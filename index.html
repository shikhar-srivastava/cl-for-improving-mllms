<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Improving MLLMs with Continual Learning">
  <meta property="og:title" content="Improving Multimodal Large Language Models Using Continual Learning"/>
  <meta property="og:description" content="This study investigates using continual learning to mitigate linguistic forgetting in Multimodal Large Language Models (MLLMs), enhancing visual understanding while preserving language skills."/>
  <meta property="og:url" content="YOUR_URL_HERE"/>

  <meta property="og:image" content="static/images/banner/mllm_cl_big_webpage.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>
  
  <meta name="twitter:title" content="Improving Multimodal Large Language Models Using Continual Learning">
  <meta name="twitter:description" content="This study investigates using continual learning to mitigate linguistic forgetting in Multimodal Large Language Models (MLLMs), enhancing visual understanding while preserving language skills.">
  <meta name="twitter:image" content="static/images/banner/mllm_cl_big_webpage.png">
  <meta name="twitter:card" content="summary_large_image">
  
  <meta name="keywords" content="Continual Learning, Multimodal Large Language Models, MLLM, LLM, Catastrophic Forgetting, LLaVA">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Improving MLLMs with Continual Learning</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" sizes="180x180" href="static/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="static/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="static/images/favicon-16x16.png">
  
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link href="https://fonts.googleapis.com/css2?family=Latin+Modern+Roman:wght@400;700&display=swap" rel="stylesheet">
  
  <style>
    .hero .title, .hero .subtitle, .title, .subtitle {
      font-family: 'Latin Modern Roman', serif;
    }
    
    /* Reduce spacing between hero sections */
    .hero {
      padding-bottom: 0;
    }
    
    .hero.teaser {
      padding-top: 0;
      margin-top: -3rem;
    }
  </style>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Improving Multimodal Large Language Models Using Continual Learning</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.cs.rochester.edu/u/ssrivas9/" target="_blank"><b>Shikhar Srivastava</b></a><sup>1*</sup>,
            </span>
            <span class="author-block">
              <a href="https://yousuf907.github.io/" target="_blank"><b>Md Yousuf Harun</b></a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://robikshrestha.com" target="_blank"><b>Robik Shrestha</b></a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://chriskanan.com" target="_blank"><b>Christopher Kanan</b></a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup> University of Rochester</span>
            <span class="author-block"><sup>2</sup> Rochester Institute of Technology</span>
          </div>
          <div class="is-size-6 publication-authors" style="margin-top: 0.5rem;">
            <span class="author-block">
                <small><sup>*</sup>Corresponding author: <a href="mailto:shikhar.srivastava@rochester.edu">shikhar.srivastava@rochester.edu</a></small>
            </span>
          </div>

                     <div class="is-flex is-justify-content-center is-align-items-center" style="gap: 1rem; margin-top: 1.5rem; flex-wrap: wrap;">
             <div class="tags has-addons" style="margin: 0;">
               <span class="tag is-dark is-medium">
                 <span class="icon"><i class="fas fa-award"></i></span>
                 <span>CoLLAs 2025</span>
               </span>
               <span class="tag is-light is-medium">Conference</span>
             </div>
             <div class="tags has-addons" style="margin: 0;">
               <span class="tag is-dark is-medium">
                 <span class="icon"><i class="fas fa-award"></i></span>
                 <span>SCFM, NeurIPS 2024</span>
               </span>
               <span class="tag is-light is-medium">Workshop</span>
             </div>
           </div>
              <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="#" class="external-link button is-normal is-rounded is-dark" style="opacity: 0.5; pointer-events: none;">
                    <span class="icon"><i class="fab fa-github"></i></span>
                    <span>Code (Coming Soon)</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2410.19925" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="ai ai-arxiv"></i></span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://lifelong-ml.cc/Conferences/2025/acceptedpapersandvideos/conf-2025-8" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-play"></i></span>
                    <span>Presentation</span>
                  </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="hero teaser">
  <div class="container is-max-desktop">
    <img class="rounded" src="static/images/banner/mllm_cl_big_webpage.png" alt="A diagram showing how continual learning can be used to improve multimodal large language models by mitigating catastrophic forgetting of linguistic abilities." style="display: block; margin-left: auto; margin-right: auto; width: 95%;">
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">

        <!-- Aim -->
        <div style="border-left: 3px solid #d5d5d5; padding: 0.75rem 1rem; margin-bottom: 1rem; background-color: #fafafa;">
          <h4 class="title is-6" style="color: #666; margin-bottom: 0.5rem; font-weight: 600;">
            <span class="icon-text">
              <span class="icon has-text-grey" style="font-size: 0.75rem;">
                <i class="fas fa-bullseye"></i>
              </span>
              <span>Aim</span>
            </span>
          </h4>
          <p class="has-text-justified" style="margin-bottom: 0; font-size: 0.95rem;">
            General multimodal learning systems should retain both <strong>linguistic (text-only)</strong> and <strong>multimodal capabilities</strong>, while continuously acquiring new multimodal capabilities.
          </p>
        </div>

        <!-- Problem -->
        <div style="border-left: 3px solid #dc3545; padding: 0.75rem 1rem; margin-bottom: 1rem; background-color: #fdf2f2;">
          <h4 class="title is-6" style="color: #dc3545; margin-bottom: 0.5rem; font-weight: 600;">
            <span class="icon-text">
              <span class="icon has-text-danger" style="font-size: 0.75rem;">
                <i class="fas fa-exclamation-triangle"></i>
              </span>
              <span>Problem</span>
            </span>
          </h4>
          <p class="has-text-justified" style="margin-bottom: 0; font-size: 0.95rem;">
            Multimodal LLMs → Undergo <strong>large linguistic forgetting</strong> i.e. loss in linguistic abilities, during multimodal training.
          </p>
        </div>

        <!-- Our Goal -->
        <div style="border-left: 3px solid #d5d5d5; padding: 0.75rem 1rem; margin-bottom: 1rem; background-color: #fafafa;">
          <h4 class="title is-6" style="color: #666; margin-bottom: 0.5rem; font-weight: 600;">
            <span class="icon-text">
              <span class="icon has-text-grey" style="font-size: 0.75rem;">
                <i class="fas fa-flag"></i>
              </span>
              <span>Our Goal</span>
            </span>
          </h4>
          <p class="has-text-justified" style="margin-bottom: 0; font-size: 0.95rem;">
            Can we <strong>mitigate linguistic forgetting efficiently</strong>?
          </p>
        </div>

        <!-- Our Approach -->
        <div style="border-left: 3px solid #28a745; padding: 0.75rem 1rem; margin-bottom: 2rem; background-color: #f2f8f2;">
          <h4 class="title is-6" style="color: #28a745; margin-bottom: 0.5rem; font-weight: 600;">
            <span class="icon-text">
              <span class="icon has-text-success" style="font-size: 0.75rem;">
                <i class="fas fa-lightbulb"></i>
              </span>
              <span>Our Approach</span>
            </span>
          </h4>
          <div class="has-text-justified" style="margin-bottom: 0; font-size: 0.95rem;">
            <ul style="margin-bottom: 0; margin-left: 1rem;">
              <li>Treat <strong>Multimodal training as a Continual Learning (CL) task</strong></li>
              <li>Employ <strong>CL methods to reduce linguistic forgetting</strong> efficiently during training</li>
            </ul>
          </div>
        </div>

        <!-- Results Banner -->
        <div class="has-text-centered" style="margin-top: 2rem;">
          <figure class="image">
            <img src="viz/banner_results.png" 
                 alt="Results comparison showing improved performance with continual learning methods" 
                 style="border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); max-width: 100%; height: auto;">
          </figure>
          <p class="has-text-grey is-size-6" style="margin-top: 0.75rem; font-style: italic; font-weight: 500;">
            Summary Results: Our best CL method compared to LLaVA 1.5 and the base unimodal LLM, at 2.8B scale.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2 has-text-centered">#1: Investigating Linguistic Forgetting in MLLMs 🔍</h2>
        
        <!-- Study Description -->
        <div class="content has-text-justified" style="margin-bottom: 2rem;">
          <p style="font-size: 1.05rem; line-height: 1.6;">
            <strong>Study:</strong> We study LLaVA1.5 with <strong>9 choices of base LLMs</strong> of varying scales and instruct-tuning: 
            <strong>2 LLaMA2 (7B)</strong> models, <strong>6 Pythia</strong> models (160M - 2.8B), and <strong>Phi2 (3B)</strong>.
          </p>
        </div>

        <!-- Linguistic Forgetting Analysis Carousel -->
        <div style="margin: 2rem 0;">
          <div id="carousel-1" class="carousel results-carousel">
            <div class="item has-text-centered">
              <figure class="image">
                <img src="viz/linguistic_forgetting_figure.png" 
                     alt="Linguistic forgetting analysis across different base LLMs" 
                     style="border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); max-width: 100%; height: auto;">
              </figure>
              <p class="has-text-grey is-size-6" style="margin-top: 0.75rem; font-style: italic; font-weight: 500;">
                Linguistic Forgetting in LLaVA with varying base LLMs
              </p>
            </div>
            <div class="item has-text-centered">
              <figure class="image" style="display: flex; justify-content: center;">
                <img src="viz/linguistic_forgetting_table.png" 
                     alt="Table showing NLG and NLU forgetting results across different base LLMs" 
                     style="border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); max-width: 80%; height: auto;">
              </figure>
              <p class="has-text-grey is-size-6" style="margin-top: 0.75rem; font-style: italic; font-weight: 500;">
                NLU and NLG forgetting across LLaVA with varying base LLMs.
              </p>
            </div>
          </div>
        </div>

        <!-- Takeaway -->
        <div style="border: 2px solid #333; border-radius: 8px; padding: 1.25rem 1.75rem; margin-top: 2rem; background-color: #fafafa; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
          <h4 class="title is-5" style="color: #333; margin-bottom: 0.75rem; font-weight: 700; text-transform: uppercase; letter-spacing: 0.5px;">
            <span class="icon-text">
              <span class="icon has-text-dark" style="font-size: 1rem;">
                <i class="fas fa-key"></i>
              </span>
              <span>Takeaway</span>
            </span>
          </h4>
          <p class="has-text-justified" style="margin-bottom: 0; font-size: 1rem; font-weight: 600; line-height: 1.5;">
            Linguistic forgetting, esp. NLG forgetting is significant, and varies with scale and type of base LLM.
            </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2 has-text-centered">#2: Mitigating Linguistic Forgetting in MLLMs 🛡️</h2>
        
        <!-- Approach -->
        <div style="border-left: 3px solid #28a745; padding: 0.75rem 1rem; margin-bottom: 1rem; background-color: #f2f8f2;">
          <h4 class="title is-6" style="color: #28a745; margin-bottom: 0.5rem; font-weight: 600;">
            <span class="icon-text">
              <span class="icon has-text-success" style="font-size: 0.75rem;">
                <i class="fas fa-cogs"></i>
              </span>
              <span>Approach</span>
            </span>
          </h4>
          <p class="has-text-justified" style="margin-bottom: 0; font-size: 0.95rem;">
            We adapt ideas from <strong>Continual Learning</strong> to train MLLMs (LLaVA): <strong>Rehearsal</strong>, <strong>LoRA (Low Rank Adaptation)</strong>, <strong>Soft Targets</strong>, <strong>Stability Gap Minimization</strong>
          </p>
        </div>

        <!-- Experimental Setup -->
        <div style="border-left: 3px solid #d5d5d5; padding: 0.75rem 1rem; margin-bottom: 2rem; background-color: #fafafa;">
          <h4 class="title is-6" style="color: #666; margin-bottom: 0.5rem; font-weight: 600;">
            <span class="icon-text">
              <span class="icon has-text-grey" style="font-size: 0.75rem;">
                <i class="fas fa-flask"></i>
              </span>
              <span>Exp. Setup</span>
            </span>
          </h4>
          <p class="has-text-justified" style="margin-bottom: 0; font-size: 0.95rem;">
            MLLMs undergo alignment pre-training per <strong>LLaVA1.5 protocol</strong>. Mitigation methods applied during <strong>Multimodal LLaVA fine-tuning</strong>.
          </p>
        </div>

        <!-- Comparisons with LLaVA -->
        <div style="margin-bottom: 2rem;">
          <h3 class="title is-4" style="color: #333; margin-bottom: 1rem;">
            <span class="icon-text">
              <span class="icon has-text-info">
                <i class="fas fa-balance-scale"></i>
              </span>
              <span><strong>Comparisons with LLaVA</strong></span>
            </span>
          </h3>
          
          <div class="content has-text-justified" style="margin-bottom: 1.5rem;">
            <p style="font-size: 1.05rem; line-height: 1.6;">
              Measuring <strong>VL Perf.</strong> and <strong>Linguistic Forgetting</strong>, across <strong>0.16B to 2.8B param scales</strong>.
            </p>
          </div>

          <!-- Image Carousel -->
          <div id="carousel-2" class="carousel results-carousel">
            <div class="item">
              <img src="viz/linguistic_forgetting_with_scale.png"
                   alt="Linguistic forgetting comparison across different model scales"
                   style="border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); width: 100%; height: auto;">
            </div>
            <div class="item">
              <img src="viz/vl_performance_with_scale.png"
                   alt="Vision-language performance comparison across different model scales"
                   style="border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); width: 100%; height: auto;">
            </div>
          </div>
        </div>

        <!-- Takeaway -->
        <div style="border: 2px solid #333; border-radius: 8px; padding: 1.25rem 1.75rem; margin-top: 2rem; background-color: #fafafa; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
          <h4 class="title is-5" style="color: #333; margin-bottom: 0.75rem; font-weight: 700; text-transform: uppercase; letter-spacing: 0.5px;">
            <span class="icon-text">
              <span class="icon has-text-dark" style="font-size: 1rem;">
                <i class="fas fa-key"></i>
              </span>
              <span>Takeaway</span>
            </span>
          </h4>
          <p class="has-text-justified" style="margin-bottom: 0; font-size: 1rem; font-weight: 600; line-height: 1.5;">
            LLaVA with mitigation methods have negligible linguistic forgetting. Match full VL Performance as model scales
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2 has-text-centered">#3: Mitigating Linguistic Forgetting in MLLMs while fine-tuning continually 🔄</h2>
        
        <!-- Approach -->
        <div style="border-left: 3px solid #28a745; padding: 0.75rem 1rem; margin-bottom: 1.5rem; background-color: #f2f8f2;">
          <h4 class="title is-6" style="color: #28a745; margin-bottom: 0.5rem; font-weight: 600;">
            <span class="icon-text">
              <span class="icon has-text-success" style="font-size: 0.75rem;">
                <i class="fas fa-cogs"></i>
              </span>
              <span>Approach</span>
            </span>
          </h4>
          <p class="has-text-justified" style="margin-bottom: 0; font-size: 0.95rem;">
            We split the <strong>LLaVA1.5 multimodal fine-tuning task into 4 groups</strong> of Vision-Language (VL) tasks, to create a <strong>LLaVA CL setup</strong>. CL methods were then applied during continual training.
          </p>
        </div>

        <!-- Continual LLaVA Setup Figure -->
        <div class="has-text-centered" style="margin: 2rem 0;">
          <figure class="image">
            <img src="viz/continual_llava.png" 
                 alt="Continual LLaVA Setup showing the sequence of multimodal tasks" 
                 style="border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); max-width: 100%; height: auto;">
          </figure>
          <p class="has-text-grey is-size-6" style="margin-top: 0.75rem; font-style: italic; font-weight: 500;">
            Continual LLaVA Setup: Sequence of tasks from LLaVA Instruct 
          </p>
        </div>

        <!-- Comparisons with LLaVA -->
        <div style="margin-bottom: 2rem;">
          <h3 class="title is-4" style="color: #333; margin-bottom: 1rem;">
            <span class="icon-text">
              <span class="icon has-text-info">
                <i class="fas fa-balance-scale"></i>
              </span>
              <span><strong>Comparisons with LLaVA</strong></span>
            </span>
          </h3>
          
          <div class="content has-text-justified" style="margin-bottom: 1.5rem;">
            <p style="font-size: 1.05rem; line-height: 1.6;">
              Measuring <strong>VL Perf.</strong> and <strong>Linguistic Forgetting</strong>, averaged over all CL tasks, across <strong>0.16B to 1.4B param scales</strong>.
            </p>
          </div>

          <!-- Continual Learning Results Carousel -->
          <div id="carousel-3" class="carousel results-carousel">
            <div class="item">
              <img src="viz/linguistic_forgetting_with_scale.png"
                   alt="Linguistic forgetting comparison across different model scales"
                   style="border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); width: 100%; height: auto;">
            </div>
            <div class="item">
              <img src="viz/task_avg_linguistic_forgetting_with_scale.png"
                   alt="Task-averaged linguistic forgetting comparison across different model scales"
                   style="border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); width: 100%; height: auto;">
            </div>
          </div>
        </div>

        <!-- Takeaway -->
        <div style="border: 2px solid #333; border-radius: 8px; padding: 1.25rem 1.75rem; margin-top: 2rem; background-color: #fafafa; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
          <h4 class="title is-5" style="color: #333; margin-bottom: 0.75rem; font-weight: 700; text-transform: uppercase; letter-spacing: 0.5px;">
            <span class="icon-text">
              <span class="icon has-text-dark" style="font-size: 1rem;">
                <i class="fas fa-key"></i>
              </span>
              <span>Takeaway</span>
            </span>
          </h4>
          <p class="has-text-justified" style="margin-bottom: 0; font-size: 1rem; font-weight: 600; line-height: 1.5;">
            mSGM + R. matches full VL perf., with negligible (or even negative) linguistic forgetting (+ve BWT) in continual LLaVA training
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-12">
        <div class="content has-text-centered">
          <h4 class="title is-5" style="margin-bottom: 1rem; color: #333;">Acknowledgements</h4>
          <p style="font-size: 1rem; margin-bottom: 2rem;">
            We thank <strong>US National Science Foundation</strong> for supporting our research.
          </p>
          <p style="font-size: 0.75rem; color: #666; line-height: 1.4;">
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page. This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>